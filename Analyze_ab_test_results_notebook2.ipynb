{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results - Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "- [Overall Conclusions](#conclusions)\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are commonly performed by data analysts and data scientists.  This project involves working to understand the results of an A/B test run by an e-commerce website.  The **GOAL** is to work through this notebook to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "ADDITIONAL NOTE:  The Udacity classroom has corresponding quiz questions associated with different sections of the below Notebook.  As a final check, all the criteria that need to be met are on the [RUBRIC](https://review.udacity.com/#!/projects/37e27304-ad47-4eb0-a1ab-8c12f60e43d0/rubric).\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# The seed is being set to assure that the same answers are obtained for the Udacity classroom quizzes.\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Quiz 1-associated section:\n",
    "\n",
    "a. Read in the dataset and look at the top few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the full A/B Test data and view the top several rows.\n",
    "df = pd.read_csv('ab_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Find the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   user_id       294478 non-null  int64 \n",
      " 1   timestamp     294478 non-null  object\n",
      " 2   group         294478 non-null  object\n",
      " 3   landing_page  294478 non-null  object\n",
      " 4   converted     294478 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n",
      "None \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "294478"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of rows in the full dataset.\n",
    "print(df.info(), \"\\n\")\n",
    "\n",
    "# OR...\n",
    "df.shape[0]\n",
    "\n",
    "# NOTE: May want to change 'timestamp' column to datetime data type.\n",
    "# df.timestamp = pd.to_datetime(df.timestamp)\n",
    "#     OR\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:  294478 rows in full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. The number of unique users in the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of unique users in the full dataset.\n",
    "df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: 290584 unique users in the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d. The proportion of unique users converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12104245244060237"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The proportion of BOTH \"control\" AND \"treatment\" UNIQUE users that converted.\n",
    "df.query('converted == 1')['user_id'].nunique() / df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: A 0.12104 proportion (12.104%) of all unique users converted in the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id\n",
      "634881    2\n",
      "637639    2\n",
      "639635    2\n",
      "642866    2\n",
      "657619    2\n",
      "         ..\n",
      "915280    2\n",
      "917949    2\n",
      "937901    2\n",
      "943137    2\n",
      "945627    2\n",
      "Name: converted, Length: 64, dtype: int64\n",
      "\n",
      " Int64Index([634881, 637639, 639635, 642866, 657619, 669913, 678722, 684279,\n",
      "            689273, 697641, 699757, 709558, 716366, 724894, 728040, 728830,\n",
      "            729669, 733339, 744247, 744411, 746310, 759632, 761744, 765231,\n",
      "            767964, 768265, 779906, 781019, 810257, 824233, 826424, 828302,\n",
      "            832110, 833113, 833424, 838320, 841046, 844946, 847832, 851651,\n",
      "            858458, 858910, 859346, 859350, 864973, 873093, 874307, 876037,\n",
      "            880932, 883687, 887357, 887573, 892397, 901109, 902109, 904340,\n",
      "            911590, 913537, 915090, 915280, 917949, 937901, 943137, 945627],\n",
      "           dtype='int64', name='user_id')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>637639</td>\n",
       "      <td>2017-01-11 23:09:52.682329</td>\n",
       "      <td>control</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>858458</td>\n",
       "      <td>2017-01-06 04:51:33.183576</td>\n",
       "      <td>control</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4961</th>\n",
       "      <td>844946</td>\n",
       "      <td>2017-01-04 07:20:58.924520</td>\n",
       "      <td>treatment</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6828</th>\n",
       "      <td>642866</td>\n",
       "      <td>2017-01-11 02:18:49.882994</td>\n",
       "      <td>treatment</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8408</th>\n",
       "      <td>699757</td>\n",
       "      <td>2017-01-06 19:15:46.467126</td>\n",
       "      <td>treatment</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290149</th>\n",
       "      <td>858910</td>\n",
       "      <td>2017-01-10 05:20:37.997730</td>\n",
       "      <td>treatment</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290950</th>\n",
       "      <td>781019</td>\n",
       "      <td>2017-01-21 15:33:08.559274</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291707</th>\n",
       "      <td>917949</td>\n",
       "      <td>2017-01-18 16:24:28.263463</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292941</th>\n",
       "      <td>874307</td>\n",
       "      <td>2017-01-10 23:53:29.996813</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293817</th>\n",
       "      <td>876037</td>\n",
       "      <td>2017-01-17 16:15:08.957152</td>\n",
       "      <td>treatment</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                   timestamp      group landing_page  converted\n",
       "846      637639  2017-01-11 23:09:52.682329    control     new_page          1\n",
       "3362     858458  2017-01-06 04:51:33.183576    control     new_page          1\n",
       "4961     844946  2017-01-04 07:20:58.924520  treatment     old_page          1\n",
       "6828     642866  2017-01-11 02:18:49.882994  treatment     old_page          1\n",
       "8408     699757  2017-01-06 19:15:46.467126  treatment     old_page          1\n",
       "...         ...                         ...        ...          ...        ...\n",
       "290149   858910  2017-01-10 05:20:37.997730  treatment     old_page          1\n",
       "290950   781019  2017-01-21 15:33:08.559274  treatment     new_page          1\n",
       "291707   917949  2017-01-18 16:24:28.263463  treatment     new_page          1\n",
       "292941   874307  2017-01-10 23:53:29.996813  treatment     new_page          1\n",
       "293817   876037  2017-01-17 16:15:08.957152  treatment     old_page          1\n",
       "\n",
       "[128 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADDITIONAL NOTE (FOR MY CURIOSITY):\n",
    "# Identify any users that converted more than once.\n",
    "user_conversion = df.groupby('user_id')['converted'].sum()\n",
    "multi_converted_users = user_conversion[user_conversion > 1]\n",
    "# REM: Pandas Series name repeated, rather than giving a DataFrame \n",
    "# column name for Boolean indexing of a Pandas Series.\n",
    "\n",
    "# Display the Series of 'user_id's of those who converted more than once.\n",
    "print(multi_converted_users)\n",
    "\n",
    "# Store and display the Series indexes (which are the 'user_id's) of those who converted more than once.\n",
    "multi_converted_user_ids = multi_converted_users.index\n",
    "print('\\n', multi_converted_user_ids)\n",
    "\n",
    "# Display the full DataFrame's rows associated with the multi-converted 'user_id's.\n",
    "df.query('user_id in @multi_converted_user_ids')\n",
    "# REM: Refer to variables in the \"query\" environment by prefixing them with an â€˜@â€™ character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>637639</td>\n",
       "      <td>2017-01-11 23:09:52.682329</td>\n",
       "      <td>control</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106312</th>\n",
       "      <td>637639</td>\n",
       "      <td>2017-01-12 06:25:18.717847</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170353</th>\n",
       "      <td>634881</td>\n",
       "      <td>2017-01-03 15:57:33.336873</td>\n",
       "      <td>treatment</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193001</th>\n",
       "      <td>634881</td>\n",
       "      <td>2017-01-18 17:44:23.130122</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                   timestamp      group landing_page  converted\n",
       "846      637639  2017-01-11 23:09:52.682329    control     new_page          1\n",
       "106312   637639  2017-01-12 06:25:18.717847  treatment     new_page          1\n",
       "170353   634881  2017-01-03 15:57:33.336873  treatment     old_page          1\n",
       "193001   634881  2017-01-18 17:44:23.130122  treatment     new_page          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specifically looking at the DataFrame rows for two of the previously identified 'multi_converted_user_ids'.\n",
    "df.query('user_id in [634881, 637639]')\n",
    "\n",
    "# NOTE:  Output is as one would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35173"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just checking the total number of those who converted vs. the number of unique users who converted.\n",
    "print(df.query('converted == 1').shape[0])\n",
    "df.query('converted == 1')['user_id'].nunique()\n",
    "\n",
    "# NOTE:  From above, the expected difference of 64 = 35237 - 35173 ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e. The number of times that `new_page` and `treatment` don't match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of \"treatment\", but not \"new_page\"  PLUS  Number of \"control\", but not \"old_page\".\n",
    "df.query('group == \"treatment\" & landing_page != \"new_page\" ').shape[0] \\\n",
    "+ df.query('group == \"control\" & landing_page != \"old_page\" ').shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: 3893 rows of the full dataset had mismatching 'group' and 'landing_page' values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Do any of the rows have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In general, one can use  df.info()  to address this question as I did above\n",
    "# (and found that none of the columns, and thus, none of the rows, contained NULL values).\n",
    "\n",
    "# More specifically, return True (value of 1) for the rows with a NULL/NaN in any column of each row.\n",
    "df.isnull().any(axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: None of the rows have missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Quiz 2-associated section:\n",
    "\n",
    "a. Create a new dataset, **df2**, that no longer contains the mismatched 'group' and 'landing_page' rows (because we cannot be sure if these rows truly received the new or old page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1965, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1928, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the DataFrame rows with \"treatment\" 'group', but not \"new_page\" 'landing_page'.\n",
    "df_TxNotNewPage = df.query('group == \"treatment\" & landing_page != \"new_page\" ')\n",
    "print(df_TxNotNewPage.shape)\n",
    "\n",
    "# Identify the rows with \"control\" 'group', but not \"old_page\" 'landing_page'.\n",
    "df_CtrlNotOldPage = df.query('group == \"control\" & landing_page != \"old_page\" ')\n",
    "df_CtrlNotOldPage.shape\n",
    "\n",
    "# NOTE: 1965 + 1928 = 3893 total rows, as expected from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294478, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(292513, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the rows with \"treatment\", but not \"new_page\", and assign to df2. \n",
    "df2 = df.drop(df_TxNotNewPage.index, axis=0)    # NOTE: w/o 'inplace=True' here.\n",
    "\n",
    "print(df.shape)   # Still 294478, as expected.\n",
    "df2.shape         # NOTE:  294478 - 1965 = 292513 , as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290585, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the rows with \"control\", but not \"old_page\", permanently from df2.\n",
    "df2.drop(df_CtrlNotOldPage.index, axis=0, inplace=True)\n",
    "\n",
    "df2.shape         # NOTE:  292513 - 1928 = 290585 , as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UDACITY:  Double Check that all the correct (\"treatment\" but not \"new_page\")\n",
    "# rows were removed;  this should be 0.\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) \\\n",
    "    == False].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check that all the correct (\"control\" but not \"old_page\")\n",
    "# rows were removed;  this should be 0.\n",
    "df2[((df2['group'] == 'control') == (df2['landing_page'] == 'old_page')) \\\n",
    "    == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: All of the mismatched 'group' and 'landing_page' rows appear to have been dropped correctly from df2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Quiz 3-associated section:\n",
    "a. How many unique **user_id**s are in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique 'user_id's in df2.\n",
    "df2.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: 290584 unique users in df2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b. What is the one **user_id** repeated in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773192    2\n",
      "630732    1\n",
      "811737    1\n",
      "797392    1\n",
      "795345    1\n",
      "         ..\n",
      "650647    1\n",
      "648598    1\n",
      "654741    1\n",
      "652692    1\n",
      "630836    1\n",
      "Name: user_id, Length: 290584, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "773192    2\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value_counts based on user_id values.\n",
    "df2_user_id_valct = df2.user_id.value_counts()\n",
    "print(df2_user_id_valct) \n",
    "\n",
    "# Boolean indexing of a Pandas Series of \"value_counts\".\n",
    "df2_user_id_valct[df2_user_id_valct > 1]\n",
    "\n",
    "# NOTE:  Alternative approaches include the use of  df2.user_id.duplicated()  \n",
    "#        OR  df2.groupby('user_id') [perhaps with a .filter() method w/ a lambda function argument?])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:  The repeated 'user_id' in df2 is 773192."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is the row information for the repeat **user_id**? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([773192], dtype='int64')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexes (in this case, user_ids) of Repeated user_id rows.\n",
    "dup_user_id_valct = df2_user_id_valct[df2_user_id_valct > 1].index\n",
    "print(dup_user_id_valct)\n",
    "\n",
    "# Display rows with repeated user_ids.\n",
    "df2.query('user_id in @dup_user_id_valct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Remove **one** of the rows with a duplicate **user_id**, but keep your dataframe as **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(290584, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the row(s) having a repeated 'user_id' value.\n",
    "df2.drop_duplicates(subset=['user_id'], inplace=True)\n",
    "\n",
    "print(df2.shape)   # Should be 290584, 5\n",
    "\n",
    "# Re-display the rows with previously repeated user_ids.\n",
    "df2.query('user_id in @dup_user_id_valct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: The one row with a repeated user_id has been dropped from df2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Quiz 4-associated section:\n",
    "\n",
    "a. What is the probability of an individual converting regardless of the page they receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 290584 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   user_id       290584 non-null  int64 \n",
      " 1   timestamp     290584 non-null  object\n",
      " 2   group         290584 non-null  object\n",
      " 3   landing_page  290584 non-null  object\n",
      " 4   converted     290584 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 13.3+ MB\n",
      "None \n",
      "\n",
      "290584\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rechecking df2 for total number of rows, uniqueness of all user_ids, and \n",
    "# data types of each column (timestamp still not \"datetime\" data type).\n",
    "print(df2.info(), '\\n')\n",
    "print(df2.user_id.nunique())\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of individual converting regardless of landing_page type.\n",
    "df2.converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: A 0.119597 proportion (11.9597%) of unique users converted regardless of 'landing_page' in the df2 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Given that an individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    127785\n",
      "1     17489\n",
      "Name: converted, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of \"control\" individual converting.\n",
    "print(df2.query('group == \"control\"')['converted'].value_counts())  # pre-check.\n",
    "\n",
    "df2.query('group == \"control\"')['converted'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: A 0.120386 proportion (12.0386%) of unique \"control\" (\"old_page\") users converted in the df2 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    128046\n",
      "1     17264\n",
      "Name: converted, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of 'treatment' individual converting.\n",
    "print(df2.query('group == \"treatment\"')['converted'].value_counts())  # pre-check.\n",
    "\n",
    "df2.query('group == \"treatment\"')['converted'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: A 0.118808 proportion (11.8808%) of unique \"treatment\" (\"new_page\") users converted in the df2 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is the probability that an individual received the new page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of an individual \"landing on\" the new_page.\n",
    "(df2.landing_page == \"new_page\").mean()\n",
    "# REM: Within the parentheses, each row of the designated column evaluates as True or False \n",
    "# and they evaluate as 1 or 0, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:  There is a 50% chance of an individual landing on the new page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Consider your results from parts (a) through (d) above, and explain below whether you think there is sufficient evidence to conclude that the new treatment page leads to more conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The conversion rate for the \"treatment/new_page\" group was less than that for the \"control/old_page\" group, and the number of individuals in each of the two groups is sufficiently large to avoid unexpected large changes in proportion values with any individual change in conversion.  Thus, from this data and analysis, there is no indication that the new treatment page leads to more conversions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "#### 1. Hypotheses:\n",
    "Based on all of the df2 data and under the assumption that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, what should the Null and Alternative hypotheses be (**$p_{old}$** and **$p_{new}$** are the converted rates for the old and new pages, respectively)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Null hypothesis, H0:â€ƒ$p_{new}$ - $p_{old}$â€ƒ<=â€ƒ0**\n",
    "â€ƒâ€ƒâ€ƒâ€ƒi.e.,â€ƒÏ€\\_ð‘›ð‘’ð‘¤ - Ï€\\_ð‘œð‘™ð‘‘â€ƒ<=â€ƒ0\n",
    "\n",
    ">**Alternative hypothesis, H1:â€ƒ$p_{new}$ - $p_{old}$â€ƒ>â€ƒ0**\n",
    "â€ƒâ€ƒi.e.,â€ƒÏ€\\_ð‘›ð‘’ð‘¤ - Ï€\\_ð‘œð‘™ð‘‘â€ƒ>â€ƒ0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Quiz 5-associated section and \"Simulated\" and \"Traditional\" p-value calculations:\n",
    "NOTE: I found the Udacity instructions for this section somewhat confusing, as indicated in my 'comments' (#) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What is the **conversion rate** for $p_{new}$ under the null? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE:  I assume that the cleaned-up df2 dataset should be used here rather than\n",
    "# the full, starting \"ab_data.csv\" (as stated in the instructions) dataset, correct?\n",
    "\n",
    "# So, IF I UNDERSTOOD THE UDACITY INSTRUCTIONS CORRECTLY,\n",
    "# \"p_new under the null\" is considered here to be the \"converted rate\" \n",
    "# for the treatment/new_page individuals REGARDLESS OF THE PAGE, which is...\n",
    "p_new_true = df2.converted.mean()\n",
    "p_new_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11880806551510564"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More generally, p_new would be calculated as follows:\n",
    "p_new = df2.query('group == \"treatment\"').converted.mean()\n",
    "\n",
    "# And this particular p_new would be what I refer to as the \"observed \n",
    "# p_new\" (which I think corresponds to Udacity's \"actual\" in j. below \n",
    "# as opposed to Udacity's \"true\" p_new above, correct?)\n",
    "p_new_obs = p_new\n",
    "p_new_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions: \n",
    "- The conversion rate for p_new under the null, p_new_true, is 0.119597 (11.9597%).\n",
    "- The \"observed\" conversion rate for the \"treatment/new_page\" group in df2, p_new_obs, is 0.118808 (11.8808%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What is the **conversion rate** for $p_{old}$ under the null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Udacity also stated to assume that \"p_old under the null\" is the \n",
    "# \"converted rate\" for the control/old_page individuals REGARDLESS\n",
    "# OF PAGE, which is...\n",
    "p_old_true = df2.converted.mean()\n",
    "p_old_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More generally, p_old would be calculated as follows:\n",
    "p_old = df2.query('group == \"control\"').converted.mean()\n",
    "\n",
    "# And this particular p_old would be what I refer to as the \"observed \n",
    "# p_old\" (which I think is Udacity's \"actual\" p_old below as opposed\n",
    "# to Udacity's \"true\" p_old above, correct?)\n",
    "p_old_obs = p_old\n",
    "p_old_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusions: \n",
    "- The conversion rate for p_old under the null, p_old_true, is 0.119597 (11.9597%).\n",
    "- The \"observed\" conversion rate for the \"control/old_page\" group in df2, p_old_obs, is 0.120386 (12.0386%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is $n_{new}$, the number of individuals in the treatment group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of \"treatment\" group individuals (REM: only unique user_ids in df2).\n",
    "n_new = df2.query('group == \"treatment\"')['user_id'].count()\n",
    "n_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: The number of \"treatment\" group individuals, n_new, is 145310."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is $n_{old}$, the number of individuals in the control group?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of \"control\" group individuals (REM: only unique user_ids in df2).\n",
    "n_old = df2.query('group == \"control\"')['user_id'].count()\n",
    "n_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: The number of \"control\" group individuals, n_old, is 145274."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Simulate $n_{new}$ transactions with a conversion rate of $p_{new}$ under the null.  Store these $n_{new}$ 1's and 0's in **new_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145310,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11797536301699814"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting \"treatment\" rows from df2.\n",
    "treatment_df = df2.query('group == \"treatment\"')\n",
    "\n",
    "# My understanding here is that Udacity is looking for a single Bootstrapped\n",
    "# \"treatment\" resample, correct?\n",
    "treatment_bootsample = treatment_df.sample(n_new, replace=True)\n",
    "new_page_converted = treatment_bootsample.converted\n",
    "\n",
    "print(new_page_converted.shape)   # Just a check.\n",
    "\n",
    "p_new_sample = new_page_converted.mean()\n",
    "p_new_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Simulate $n_{old}$ transactions with a conversion rate of $p_{old}$ under the null.  Store these $n_{old}$ 1's and 0's in **old_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145274,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12163910954472239"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Starting \"control\" rows from df2.\n",
    "control_df = df2.query('group == \"control\"')\n",
    "\n",
    "# My understanding here is that Udacity is looking for a single Bootstrapped\n",
    "# \"control\" resample, correct?\n",
    "control_bootsample = control_df.sample(n_old, replace=True)\n",
    "old_page_converted = control_bootsample.converted\n",
    "\n",
    "print(old_page_converted.shape)   # Just a check.\n",
    "\n",
    "p_old_sample = old_page_converted.mean()\n",
    "p_old_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Find $p_{new}$ - $p_{old}$ for your simulated values from part (e) and (f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0036637465277242487"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The difference between the resampled treatment/new_page converted rate\n",
    "# and the resampled control/old_page converted rate.\n",
    "p_diff = p_new_sample - p_old_sample\n",
    "p_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Create 10,000 $p_{new}$ - $p_{old}$ values using the same simulation process you used in parts (a) through (g) above. Store all 10,000 values in a NumPy array called **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00108284, -0.00165389, -0.0017299 , ..., -0.00115841,\n",
       "       -0.00233525, -0.00201884])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy array to store Bootstrapped p_diff values.\n",
    "p_diffs = np.array([])\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    # new_page converted probability/proportion.\n",
    "    treatment_bootsample = treatment_df.sample(n_new, replace=True)\n",
    "    new_page_converted = treatment_bootsample.converted\n",
    "    p_new_sample = new_page_converted.mean()\n",
    "    \n",
    "    # old_page converted probability/proportion.\n",
    "    control_bootsample = control_df.sample(n_old, replace=True)\n",
    "    old_page_converted = control_bootsample.converted\n",
    "    p_old_sample = old_page_converted.mean()\n",
    "    \n",
    "    # Difference between probability of new_page converted and \n",
    "    # old_page converted.\n",
    "    p_diff = p_new_sample - p_old_sample\n",
    "    \n",
    "    # Storage of each p_diff value in p_diffs NumPy array.\n",
    "    p_diffs = np.append(p_diffs, p_diff)\n",
    "\n",
    "p_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Plot a histogram of the **p_diffs**.  Does this plot look like what you expected?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR70lEQVR4nO3df6xc5X3n8fenJqH0BwqUC+vY3jWNXGkBNaR4XVbZP7KlDVaoaqJuJEfaYmkjuUVEaqVGu6ZZqeQPSyTZNhLahV1XiTDatMhtGmE10JRY3a0qsSEXlmAMcXGCG27sxTepViW7WiSc7/5xH6Spn/Gd8b0zd67j90sazZnveZ455zsD+njOOTM3VYUkSYN+ZNY7IElafwwHSVLHcJAkdQwHSVLHcJAkdS6b9Q6Mcs0119TWrVtnvRuSdFF55plnvltVcyudv+7DYevWrczPz896NyTpopLkb1cz38NKkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqSO4SBJ6hgOkqTOuv+GtDTK1n1fmsl2T95/x0y2K60FPzlIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpMzIckvxokqeTfD3JsSSfaPWrkzyZ5OV2f9XAnHuTnEhyPMntA/Vbkhxt6x5Ikum0JUlajXE+ObwB/EJVvRu4GdiZ5FZgH3CkqrYBR9pjktwA7AZuBHYCDybZ0J7rIWAvsK3ddk6uFUnSpIwMh1ry/fbwbe1WwC7gYKsfBO5sy7uAR6vqjap6BTgB7EiyEbiyqp6qqgIeGZgjSVpHxjrnkGRDkueAM8CTVfVV4LqqOg3Q7q9twzcBrw5MX2i1TW353Pqw7e1NMp9kfnFx8QLakSRNwljhUFVnq+pmYDNLnwJuWmb4sPMItUx92PYOVNX2qto+Nzc3zi5Kkibogq5Wqqr/Dfw3ls4VvNYOFdHuz7RhC8CWgWmbgVOtvnlIXZK0zoxztdJckne05SuAXwS+ARwG9rRhe4DH2vJhYHeSy5Ncz9KJ56fboafXk9zarlK6a2COJGkdGedXWTcCB9sVRz8CHKqqP0vyFHAoyUeAbwMfAqiqY0kOAS8CbwL3VNXZ9lx3Aw8DVwBPtJskaZ0ZGQ5V9TzwniH17wG3nWfOfmD/kPo8sNz5CknSOuA3pCVJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQxHCRJHcNBktQZGQ5JtiT5yyQvJTmW5Ddb/b4k30nyXLt9YGDOvUlOJDme5PaB+i1JjrZ1DyTJdNqSJK3GZWOMeRP47ap6NslPAs8kebKt+0xV/YfBwUluAHYDNwLvBL6S5Geq6izwELAX+B/A48BO4InJtCJJmpSRnxyq6nRVPduWXwdeAjYtM2UX8GhVvVFVrwAngB1JNgJXVtVTVVXAI8Cdq21AkjR5F3TOIclW4D3AV1vpo0meT/K5JFe12ibg1YFpC622qS2fWx+2nb1J5pPMLy4uXsguSpImYOxwSPITwBeA36qqv2fpENG7gJuB08DvvTV0yPRapt4Xqw5U1faq2j43NzfuLkqSJmSscEjyNpaC4fNV9acAVfVaVZ2tqh8AfwDsaMMXgC0D0zcDp1p985C6JGmdGedqpQCfBV6qqt8fqG8cGPZB4IW2fBjYneTyJNcD24Cnq+o08HqSW9tz3gU8NqE+JEkTNM7VSu8Ffg04muS5Vvsd4MNJbmbp0NBJ4NcBqupYkkPAiyxd6XRPu1IJ4G7gYeAKlq5S8kolXbS27vvSzLZ98v47ZrZtXRpGhkNV/TXDzxc8vsyc/cD+IfV54KYL2UFJ0trzG9KSpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqjPM3pKWRZvn3lCVNnp8cJEkdw0GS1DEcJEkdw0GS1DEcJEmdkeGQZEuSv0zyUpJjSX6z1a9O8mSSl9v9VQNz7k1yIsnxJLcP1G9JcrSteyBJptOWJGk1xvnk8Cbw21X1T4FbgXuS3ADsA45U1TbgSHtMW7cbuBHYCTyYZEN7roeAvcC2dts5wV4kSRMyMhyq6nRVPduWXwdeAjYBu4CDbdhB4M62vAt4tKreqKpXgBPAjiQbgSur6qmqKuCRgTmSpHXkgs45JNkKvAf4KnBdVZ2GpQABrm3DNgGvDkxbaLVNbfnc+rDt7E0yn2R+cXHxQnZRkjQBY4dDkp8AvgD8VlX9/XJDh9RqmXpfrDpQVduravvc3Ny4uyhJmpCxwiHJ21gKhs9X1Z+28mvtUBHt/kyrLwBbBqZvBk61+uYhdUnSOjPO1UoBPgu8VFW/P7DqMLCnLe8BHhuo705yeZLrWTrx/HQ79PR6klvbc941MEeStI6M88N77wV+DTia5LlW+x3gfuBQko8A3wY+BFBVx5IcAl5k6Uqne6rqbJt3N/AwcAXwRLtJktaZkeFQVX/N8PMFALedZ85+YP+Q+jxw04XsoCRp7fkNaUlSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSx3CQJHUMB0lSZ2Q4JPlckjNJXhio3ZfkO0mea7cPDKy7N8mJJMeT3D5QvyXJ0bbugSSZfDuSpEkY55PDw8DOIfXPVNXN7fY4QJIbgN3AjW3Og0k2tPEPAXuBbe027DklSevAyHCoqr8C/m7M59sFPFpVb1TVK8AJYEeSjcCVVfVUVRXwCHDnCvdZkjRlqznn8NEkz7fDTle12ibg1YExC622qS2fWx8qyd4k80nmFxcXV7GLkqSVWGk4PAS8C7gZOA38XqsPO49Qy9SHqqoDVbW9qrbPzc2tcBclSSu1onCoqteq6mxV/QD4A2BHW7UAbBkYuhk41eqbh9QlSevQisKhnUN4yweBt65kOgzsTnJ5kutZOvH8dFWdBl5Pcmu7Suku4LFV7LckaYouGzUgyR8B7wOuSbIA/C7wviQ3s3Ro6CTw6wBVdSzJIeBF4E3gnqo6257qbpaufLoCeKLdJEnr0MhwqKoPDyl/dpnx+4H9Q+rzwE0XtHeSpJnwG9KSpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqGA6SpI7hIEnqXDbrHZB04bbu+9JMtnvy/jtmsl2tPT85SJI6I8MhyeeSnEnywkDt6iRPJnm53V81sO7eJCeSHE9y+0D9liRH27oHkmTy7UiSJmGcTw4PAzvPqe0DjlTVNuBIe0ySG4DdwI1tzoNJNrQ5DwF7gW3tdu5zSpLWiZHhUFV/BfzdOeVdwMG2fBC4c6D+aFW9UVWvACeAHUk2AldW1VNVVcAjA3MkSevMSs85XFdVpwHa/bWtvgl4dWDcQqttasvn1odKsjfJfJL5xcXFFe6iJGmlJn1Ceth5hFqmPlRVHaiq7VW1fW5ubmI7J0kaz0rD4bV2qIh2f6bVF4AtA+M2A6daffOQuiRpHVppOBwG9rTlPcBjA/XdSS5Pcj1LJ56fboeeXk9ya7tK6a6BOZKkdWbkl+CS/BHwPuCaJAvA7wL3A4eSfAT4NvAhgKo6luQQ8CLwJnBPVZ1tT3U3S1c+XQE80W6SpHVoZDhU1YfPs+q284zfD+wfUp8HbrqgvZMkzYTfkJYkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVLHcJAkdQwHSVJn5M9n6OIyqz88L+mHi58cJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEkdw0GS1DEcJEmdVYVDkpNJjiZ5Lsl8q12d5MkkL7f7qwbG35vkRJLjSW5f7c5LkqZjEp8c/mVV3VxV29vjfcCRqtoGHGmPSXIDsBu4EdgJPJhkwwS2L0masGkcVtoFHGzLB4E7B+qPVtUbVfUKcALYMYXtS5JWabXhUMBfJHkmyd5Wu66qTgO0+2tbfRPw6sDchVbrJNmbZD7J/OLi4ip3UZJ0oVb7l+DeW1WnklwLPJnkG8uMzZBaDRtYVQeAAwDbt28fOkaSND2r+uRQVafa/RngiywdJnotyUaAdn+mDV8AtgxM3wycWs32JUnTseJwSPLjSX7yrWXg/cALwGFgTxu2B3isLR8Gdie5PMn1wDbg6ZVuX5I0Pas5rHQd8MUkbz3PH1bVnyf5GnAoyUeAbwMfAqiqY0kOAS8CbwL3VNXZVe29JGkqVhwOVfUt4N1D6t8DbjvPnP3A/pVuU5K0NvyGtCSpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySpYzhIkjqGgySps9q/5yDpErJ135dmst2T998xk+1eyvzkIEnqGA6SpI7hIEnqGA6SpI7hIEnqeLXSFMzqig5JmhQ/OUiSOoaDJKljOEiSOoaDJKljOEiSOoaDJKmz5uGQZGeS40lOJNm31tuXJI22pt9zSLIB+E/ALwELwNeSHK6qF9dyPyRdXGb53aFL9Rdh1/pLcDuAE1X1LYAkjwK7gKmEg19Gk6SVWetw2AS8OvB4Afj5cwcl2QvsbQ+/n+T4BLZ9DfDdCTzPxeZS7Rsu3d4v1b5hCr3nk5N8tqkZ1vc/Wc0TrnU4ZEitukLVAeDARDeczFfV9kk+58XgUu0bLt3eL9W+4dLtfRp9r/UJ6QVgy8DjzcCpNd4HSdIIax0OXwO2Jbk+yduB3cDhNd4HSdIIa3pYqareTPJR4MvABuBzVXVsjTY/0cNUF5FLtW+4dHu/VPuGS7f3ifedqu6QvyTpEuc3pCVJHcNBktS5qMMhydVJnkzycru/6jzjhv5kx3Lzk/xskqeSHEtyNMmPrkVP45pm7239P07y/SQfm3YvF2JafSf5pSTPtPf6mSS/sFY9jTLqJ2ey5IG2/vkkPzdq7riv4yxNqe9PJ/lGG//FJO9Yo3YuyDR6H1j/sSSV5Jpld6KqLtob8ClgX1veB3xyyJgNwDeBnwbeDnwduGG5+SydqH8eeHd7/FPAhln3uxa9D8z9AvDHwMdm3esavefvAd7Zlm8CvjPrXkf1MjDmA8ATLH2P6Fbgq6t9/2d9m2Lf7wcua8ufXG99T7P3tn4LSxcE/S1wzbL7MesXYpUv4nFgY1veCBwfMuafA18eeHwvcO9y89sL/19n3d8sem+P7wQ+DdzH+guHqfU9MD7A94DL10G/5+1loPZfgA+f+xqt9nX4Yez7nPkfBD4/617XsnfgT4B3AycZEQ4X9WEl4LqqOg3Q7q8dMmbYT3ZsGjH/Z4BK8uUkzyb5t1PZ+9WZSu9Jfhz4d8AnprTfqzWt93zQrwL/s6remNher9xyvYwas9rXYZam1fegf8PSv77Xm6n0nuRXWPpE/PVxdmKtfz7jgiX5CvCPhqz6+LhPMaQ26vrdy4B/Afwz4P8CR5I8U1VHxtzmRMyo908An6mq7yfDpk/fjPp+a9s3snS44f1jbmvaxunlfGNW/DqsA1PtO8nHgTeBz69o76Zr4r0n+TGW/v8Z+7/rdR8OVfWL51uX5LUkG6vqdJKNwJkhw5b7yY7zzV8A/ntVfbdt53Hg54A1DYcZ9f7zwL9K8ingHcAPkvy/qvqPq+1nXDPqmySbgS8Cd1XVN1fdyGSM85Mz5xvz9mXmjvM6ztK0+ibJHuCXgduqHWtZZ6bR+7uA64Gvt3/0bQaeTbKjqv7X0L2Y9fG1VR6b+zT/8KTap4aMuQz4Vnth3jpBc+Ny84GrgGeBH2vzvwLcMet+16L3c+bfx/o75zCt9/wdbdyvzrrHcXsZGHMH//Dk5NOTeP9/SPveydKfCJibdY9r3fs580/yQ35C+qdY+tf8y+3+6lZ/J/D4wLgPAH/D0ln8j4+a39b9a+AY8MJ6+x9n2r0PjLmP9RcOU+kb+PfA/wGeG7hdO+t+z9cL8BvAb7TlsPRHtL4JHAW2T+L9n/VtSn2fYOmY/Fvv8X+edZ9r1fs5z3+SEeHgz2dIkjoX+9VKkqQpMBwkSR3DQZLUMRwkSR3DQZLUMRwkSR3DQZLU+f8wIcJ9ZSje7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the distribution of the statistic of interest, i.e., p-diff.\n",
    "plt.hist(p_diffs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:â€ƒAs expected for a difference in proportions, the distribution of this statistic appears to be \"Normal\" in shape.  Thus, we can create a Normal distribution to \"Simulate from the Null\" and then assess the probability of the observed p_diff statistic value with respect to the Null Space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. What proportion of the **p_diffs** are greater than the actual (observed) difference observed in **ab_data.csv (i.e., df2)**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0939"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADDITIONAL NOTE (FOR MY CURIOSITY):  Here, I am taking the \"actual difference observed in \n",
    "# ab_data.csv\" as still referring to the assumed Null hypothesis\n",
    "# difference of zero (0).  THIS IS NOT HOW WE HAVE DONE THIS IN THE PAST,\n",
    "# but I am interested in this value and how it compares to the calculated p-value further below.\n",
    "(p_diffs > 0).mean()\n",
    "\n",
    "# NOTE: This value, at least in this instance, does appear to be \n",
    "# quite close to  1 - (p-value) , with \"p-value\" as calculated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0015782389853555567\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create distribution under the Null hypothesis (i.e., Simulate from\n",
    "# the Null).\n",
    "null_vals = np.random.normal(loc=0, scale=p_diffs.std(), size=p_diffs.size)\n",
    "\n",
    "# Calculate the actual/observed difference, p_diff_obs, from above p_new_obs and p_old_obs.\n",
    "p_diff_obs = p_new_obs - p_old_obs\n",
    "print(p_diff_obs)\n",
    "\n",
    "# Histogram of the Simulated Null Space with a vertical line for the\n",
    "# observed difference in converted rate for the new page vs. the old page.\n",
    "plt.hist(null_vals);\n",
    "plt.axvline(x=p_diff_obs, color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9051"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p-value calculation of the observed difference in\n",
    "# converted rate for the new page vs. the old page.\n",
    "(null_vals > p_diff_obs).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k. Please explain using the vocabulary you've learned in this course what you just computed in part **j.**  What is this value called in scientific studies?  What does this value mean in terms of whether or not there is a difference between the new and old pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The value calculated in part j. is the _p-value_ of the observed sample statistic, p_diff_obs (-0.001578).  This p-value (~0.90) is the probability of observing this statistic value or one more extreme in favor of (i.e., toward) the Alternative Hypothesis, given that the Null Hypothesis is true.**\n",
    "\n",
    "**Thus, the very high probability (~0.90) of observing this statistic value indicates that the Null Hypothesis should be retained.  That is, we cannot conclude from this data that the new page is significantly more likely to have a higher \"converted rate\" than the old page.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l. Use a \"traditional\" built-in (closed-form) function to achieve similar results to the above \"Simulating from the Null\" approach, the latter of which is critical to correctly think about statistical significance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17489\n",
      "17264\n",
      "145274\n",
      "145310\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Determine the number of conversions for \"control/old_page\" and for \"treatment/new_page\".\n",
    "convert_old = df2.query('group == \"control\"').converted.sum()\n",
    "convert_new = df2.query('group == \"treatment\"').converted.sum()\n",
    "\n",
    "# Determine the number of rows associated with \"control/old_page\" and \"treatment/new_page\".\n",
    "n_old = df2.query('group == \"control\"').user_id.count()\n",
    "n_new = df2.query('group == \"treatment\"').user_id.count()\n",
    "\n",
    "print(convert_old)\n",
    "print(convert_new)\n",
    "print(n_old)\n",
    "print(n_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m. Now use `stats.proportions_ztest` to compute the test statistic and p-value from the \"single\" starting (non-resampled) df2 dataset.  [Here](https://docs.w3cub.com/statsmodels/generated/statsmodels.stats.proportion.proportions_ztest/) is a helpful link on using this statsmodels built-in function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3109241984234394, 0.9050583127590245)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Difference in Two (Large Sample) Proportions Z-test.\n",
    "counts = np.array([convert_new, convert_old])\n",
    "nobs = np.array([n_new, n_old])\n",
    "# NOTE: 'counts' and 'nobs' ordered above so that p_new is \"p1\" and\n",
    "# p_old is \"p2\" with respect to the stats.proportions_ztest function. \n",
    "\n",
    "stat, pval = sm.stats.proportions_ztest(counts, nobs, value=0, alternative='larger')\n",
    "# NOTE: 'larger' indicates that the Alternative Hypothesis is p1 > p2, i.e.,\n",
    "# p_new - p_old > 0, as indicated above for H1.\n",
    "\n",
    "stat, pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n. What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages?  Do they agree with the findings in parts **j.** and **k.**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The z-score (-1.3109) is calculated by dividing the difference between the two sample proportions, i.e., p_new - p_old, by the appropriate Standard Error [(Stat Trek)](https://stattrek.com/hypothesis-test/difference-in-proportions.aspx?tutorial=AP). This z-score represents the \"standardized\" distance from (i.e., \"to the left of\", because negative) the central value, 0, of a Normal-shaped distribution curve, and it is derived from the non-standardized p\\_diff\\_obs value (-0.001578) above.**\n",
    "\n",
    "**From a Probability Table for the Standard Normal Distribution, a z-score of -1.3109 indicates a probability area of just over 0.40 from the observed z-score to the central value (i.e., toward H1, because p\\_new _greater than_ p\\_old). Adding this to the 0.50 probability area to the right of the central value (i.e., again, _greater than_), one ends up with a probability of ~0.90 (90%) that the observed statistic or one more extreme toward H1 would occur under the Null Hypothesis (i.e., would fall within the theoretical Null Space).**\n",
    "\n",
    "**Thus, both the \"traditional\" z-score and its associated p-value are in agreement with the \"simulated\" findings in parts j. and k.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "#### 1. Regression approach to obtain a similar result to that in the A/B Test, Part II (above).<br>\n",
    "\n",
    "a. Since each row is either a conversion or no conversion, what type of regression should be performed in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression should be performed in this case because the 'converted' response(y)-variable values are binary categorical (0 or 1), rather than quantitative, in nature.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Use **statsmodels** to fit a Logistic Regression model to see if there is a significant difference in conversion based on which page a customer receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page  \n",
       "0          1        0  \n",
       "1          1        0  \n",
       "2          1        1  \n",
       "3          1        1  \n",
       "4          1        0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add necessary columns to the DataFrame:\n",
    "\n",
    "# (1) Add an 'intercept' column.\n",
    "df2['intercept'] = 1\n",
    "\n",
    "# (2) Convert the 'converted' binary categorical response(y) variable\n",
    "# values to  0s and 1s using .getdummies(), if needed.\n",
    "# NOTE: Already 0s and 1s for non-converted and converted, respectively.\n",
    "\n",
    "# (3) Generate dummy variable columns from categorical explanatory(x)\n",
    "# column(s) using .getdummies(). \n",
    "# In this case, the only explanatory variable being considered is the 'landing_page', \n",
    "# and we want the \"treatment\" individuals to be coded as 1s and \"control\" individuals as 0s.\n",
    "# REM: Output is in alphabetical order of the column values.\n",
    "df2[['control_page', 'ab_page']] = pd.get_dummies(df2['group'])\n",
    "\n",
    "# (4) Generate higher-order explanatory(x) term columns, if desired.\n",
    "# NOTE: Not applicable here.\n",
    "\n",
    "# Drop extra column(s), if desired.\n",
    "df2 = df2.drop('control_page', axis=1)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use **statsmodels** to instantiate (set up) and fit a Logistic Regression model based on the 'converted' response(y)-variable column and all but one (the \"baseline\") of the categorical explanatory(x)-variable dummy columns (along with the 'intercept' column) to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "# Set up the Logistic Regression model.\n",
    "logit_model = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\n",
    "\n",
    "# Fit the model.\n",
    "results = logit_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Provide the summary of your model below, and use it as necessary to answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290582</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 31 Mar 2021</td> <th>  Pseudo R-squ.:     </th>  <td>8.077e-06</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:16:49</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1899</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9888</td> <td>    0.008</td> <td> -246.669</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0150</td> <td>    0.011</td> <td>   -1.311</td> <td> 0.190</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290582\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Wed, 31 Mar 2021   Pseudo R-squ.:               8.077e-06\n",
       "Time:                        13:16:49   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1899\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
       "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of the model fit results.\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What is the p-value associated with **ab_page**? Why does it differ from the value you found in **Part II**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The p-value associated with 'ab_page' is 0.190.**\n",
    "\n",
    "**For the regression model, the calculated p-value is a \"two-sided\" p-value, rather than the \"one-sided\" p-value of the Part II A/B Test.  That is, the regression model is based on a Null Hypothesis (H0) of  p_new - p_old = 0  and an Alternative Hypothesis (H1) of  p_new - p_old != 0.  Thus, with the regression model, when we ask \"What is the probability of the observed statistic or one more extreme toward H1, given that the Null Hypothesis is true?\", we assess the area-under-the-curve further away than the z-score _in either direction_ (i.e., < -1.311 or > +1.311 standardized units) from the central value, 0, of the Standard Normal Distribution.  So, in this case, the Probability Table of the Standard Normal Distribution indicates that the \"more extreme\" areas associated with a z-score of |-1.311| are (2)(0.095) = 0.190 .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9851119396030626"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADDITIONAL NOTE (FOR MY CURIOSITY): \"Exponentiate\" the explanatory(x) variable coefficient(s) to determine... \n",
    "# \"the number of times as likely the 'treatment/new_page(1)' is to 'convert' \n",
    "# (i.e., odds of a '1' value) compared to the 'control/old_page(0)' (i.e., the \"Baseline\",\n",
    "# which corresponds to the Dummy variable that was dropped in the regression model set up).\n",
    "np.exp(-0.015)\n",
    "\n",
    "# NOTE: So, 0.985 indicates that it is slightly less likely to 'convert' with the new_page\n",
    "# compared to the old_page, which is consistent with p_diff_obs (-0.001578) further above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. With respect to considering other things that might influence whether or not an individual converts, discuss why it is a good idea to consider other factors to add into your regression model.  Are there any disadvantages to adding additional terms into your regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A/B Testing and the preceding ab_page-only regression model essentially focused on an \"aggregated\" (overall) assessment of whether the new page led to a greater converted rate than the old page, so these approaches do not address what other \"features\" might contribute to an individual deciding to convert.  Thus, by considering other factors/features in the modeling, one may be able to begin to identify additional underlying factors contributing to an individual converting.**\n",
    "\n",
    "**In general, adding additional features to a model may improve fitting; however, a disadvantage of doing this (particularly with higher-order features) is that interpretation of the results can become more difficult.  In addition, one needs to keep in mind that even if a particular feature does appear to correlate with converting, this may be an indirect effect (i.e., \"correlation does not mean causation\").**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Now, along with testing if the conversion rate changes for different landing pages, also add an effect based on which country a user lives in (see [Here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) for how to join the **countries.csv** dataset with df2 based on the appropriate column(s) values). Does it appear that country had an impact on conversion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 290584 entries, 0 to 290583\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   user_id  290584 non-null  int64 \n",
      " 1   country  290584 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>834778</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>928468</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>822059</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>711597</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>710616</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id country\n",
       "0   834778      UK\n",
       "1   928468      US\n",
       "2   822059      UK\n",
       "3   711597      UK\n",
       "4   710616      UK"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in countries.csv data, and check on the column names and number of rows.\n",
    "country_df = pd.read_csv('countries.csv')\n",
    "\n",
    "print(country_df.info())\n",
    "country_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "\n",
       "   intercept  ab_page country  \n",
       "0          1        0      US  \n",
       "1          1        0      US  \n",
       "2          1        1      US  \n",
       "3          1        1      US  \n",
       "4          1        0      US  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join/Merge df2 and country_df based on 'user_id'.\n",
    "df3 = df2.join(country_df.set_index('user_id'), on='user_id')\n",
    "\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[834778 928468 822059 711597 710616]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48778</th>\n",
       "      <td>711597</td>\n",
       "      <td>2017-01-22 03:14:24.763511</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106686</th>\n",
       "      <td>710616</td>\n",
       "      <td>2017-01-16 13:14:44.000513</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143206</th>\n",
       "      <td>834778</td>\n",
       "      <td>2017-01-14 23:08:43.304998</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157345</th>\n",
       "      <td>928468</td>\n",
       "      <td>2017-01-23 14:44:16.387854</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257177</th>\n",
       "      <td>822059</td>\n",
       "      <td>2017-01-16 14:04:14.719771</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                   timestamp      group landing_page  \\\n",
       "48778    711597  2017-01-22 03:14:24.763511    control     old_page   \n",
       "106686   710616  2017-01-16 13:14:44.000513  treatment     new_page   \n",
       "143206   834778  2017-01-14 23:08:43.304998    control     old_page   \n",
       "157345   928468  2017-01-23 14:44:16.387854  treatment     new_page   \n",
       "257177   822059  2017-01-16 14:04:14.719771  treatment     new_page   \n",
       "\n",
       "        converted  intercept  ab_page country  \n",
       "48778           0          1        0      UK  \n",
       "106686          0          1        1      UK  \n",
       "143206          0          1        0      UK  \n",
       "157345          0          1        1      US  \n",
       "257177          1          1        1      UK  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that the expected country was aligned based on 'user_id'.\n",
    "\n",
    "# First five 'user_id' values from country_df above.\n",
    "ids = country_df.user_id.values[0:5]\n",
    "print(ids)\n",
    "\n",
    "df3.query('user_id in @ids')\n",
    "\n",
    "# NOTE: Looks correct, UK for all but user_id 928468."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UK', 'US', 'CA'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique values in the 'country' column.\n",
    "country_df.country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>936923</td>\n",
       "      <td>2017-01-10 15:20:49.083499</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>679687</td>\n",
       "      <td>2017-01-19 03:26:46.940749</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>719014</td>\n",
       "      <td>2017-01-17 01:48:29.539573</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>817355</td>\n",
       "      <td>2017-01-04 17:58:08.979471</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>839785</td>\n",
       "      <td>2017-01-15 18:11:06.610965</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "5   936923  2017-01-10 15:20:49.083499    control     old_page          0   \n",
       "6   679687  2017-01-19 03:26:46.940749  treatment     new_page          1   \n",
       "7   719014  2017-01-17 01:48:29.539573    control     old_page          0   \n",
       "8   817355  2017-01-04 17:58:08.979471  treatment     new_page          1   \n",
       "9   839785  2017-01-15 18:11:06.610965  treatment     new_page          1   \n",
       "\n",
       "   intercept  ab_page country  CA  UK  US  \n",
       "0          1        0      US   0   0   1  \n",
       "1          1        0      US   0   0   1  \n",
       "2          1        1      US   0   0   1  \n",
       "3          1        1      US   0   0   1  \n",
       "4          1        0      US   0   0   1  \n",
       "5          1        0      US   0   0   1  \n",
       "6          1        1      CA   1   0   0  \n",
       "7          1        0      US   0   0   1  \n",
       "8          1        1      UK   0   1   0  \n",
       "9          1        1      CA   1   0   0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dummy variable columns from the 'country' column and add to df3.\n",
    "# REM: Output is in alphabetical order of the column values.\n",
    "df3[['CA', 'UK', 'US']] = pd.get_dummies(df3['country'])\n",
    "\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 31 Mar 2021</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:16:51</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9893</td> <td>    0.009</td> <td> -223.763</td> <td> 0.000</td> <td>   -2.007</td> <td>   -1.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>   -0.0408</td> <td>    0.027</td> <td>   -1.516</td> <td> 0.130</td> <td>   -0.093</td> <td>    0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>    0.0099</td> <td>    0.013</td> <td>    0.743</td> <td> 0.457</td> <td>   -0.016</td> <td>    0.036</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Wed, 31 Mar 2021   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        13:16:51   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1760\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9893      0.009   -223.763      0.000      -2.007      -1.972\n",
       "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "CA            -0.0408      0.027     -1.516      0.130      -0.093       0.012\n",
       "UK             0.0099      0.013      0.743      0.457      -0.016       0.036\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up Logistic Regression model.\n",
    "# REM: Drop one of each explanatory(x)-variable dummy columns.\n",
    "logit_model = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'CA', 'UK']])\n",
    "# REM: \"Baseline\" for 'ab-page' is old_page and for the three countries it is US, as set up here.\n",
    "\n",
    "# Fit and Summarize model.\n",
    "results = logit_model.fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290580</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     3</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 31 Mar 2021</td> <th>  Pseudo R-squ.:     </th>  <td>2.323e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:16:52</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1760</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -2.0300</td> <td>    0.027</td> <td>  -76.249</td> <td> 0.000</td> <td>   -2.082</td> <td>   -1.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0149</td> <td>    0.011</td> <td>   -1.307</td> <td> 0.191</td> <td>   -0.037</td> <td>    0.007</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>    0.0506</td> <td>    0.028</td> <td>    1.784</td> <td> 0.074</td> <td>   -0.005</td> <td>    0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>    0.0408</td> <td>    0.027</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.012</td> <td>    0.093</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290580\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Wed, 31 Mar 2021   Pseudo R-squ.:               2.323e-05\n",
       "Time:                        13:16:52   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1760\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -2.0300      0.027    -76.249      0.000      -2.082      -1.978\n",
       "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
       "UK             0.0506      0.028      1.784      0.074      -0.005       0.106\n",
       "US             0.0408      0.027      1.516      0.130      -0.012       0.093\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For comparison, set up a second Logistic Regression model with CA (Canada) as the 'country' baseline.\n",
    "logit_model = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK', 'US']])\n",
    "# REM: \"Baseline\" for 'ab-page' is old_page and for the three countries it is CA (Canada), as set up here.\n",
    "\n",
    "# Fit and Summarize model.\n",
    "results = logit_model.fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0519020483004984, 1.0416437559600236)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADDITIONAL NOTE (FOR MY CURIOSITY): \"Exponentiate\" the explanatory(x) variable coefficient(s) to determine... \n",
    "# \"the number of times as likely UK or US individuals are to 'convert' \n",
    "# (i.e., odds of a '1' value) compared to CA individuals (i.e., the \"Baseline\",\n",
    "# which corresponds to the Dummy variable that was dropped in the regression model set up).\n",
    "np.exp(0.0506), np.exp(0.0408)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group      country\n",
       "control    CA         0.118783\n",
       "           UK         0.120022\n",
       "           US         0.120630\n",
       "treatment  CA         0.111902\n",
       "           UK         0.121171\n",
       "           US         0.118466\n",
       "Name: converted, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converted rates for control/old_page and treatment/new_page groups based on country\n",
    "# for comparison to the preceding Logit Regression results.\n",
    "df3.groupby(['group', 'country'])['converted'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:  Does it appear that country had an impact on conversion?**\n",
    "\n",
    "**In the (second) modeling that includes 'country', the p-values 0.074 and 0.130 respectively correspond to the probability of conversion in the UK and US compared to the \"baseline\", CA. These two-sided p-values do not quite meet the typical 0.05 alpha level of significance, but the data is suggestive of a higher converted rate in the UK (with a statistically significant p-value under the appropriate one-sided hypotheses), and perhaps in the US, compared to CA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Having looked at the individual factors of country and landing page on conversion, now look at an \"Interaction\" between page and country to see if there are significant effects on conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "      <th>intercept</th>\n",
       "      <th>ab_page</th>\n",
       "      <th>country</th>\n",
       "      <th>CA</th>\n",
       "      <th>UK</th>\n",
       "      <th>US</th>\n",
       "      <th>page_CA</th>\n",
       "      <th>page_UK</th>\n",
       "      <th>page_US</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>936923</td>\n",
       "      <td>2017-01-10 15:20:49.083499</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>679687</td>\n",
       "      <td>2017-01-19 03:26:46.940749</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>719014</td>\n",
       "      <td>2017-01-17 01:48:29.539573</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>817355</td>\n",
       "      <td>2017-01-04 17:58:08.979471</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>UK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>839785</td>\n",
       "      <td>2017-01-15 18:11:06.610965</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted  \\\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0   \n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0   \n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0   \n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0   \n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1   \n",
       "5   936923  2017-01-10 15:20:49.083499    control     old_page          0   \n",
       "6   679687  2017-01-19 03:26:46.940749  treatment     new_page          1   \n",
       "7   719014  2017-01-17 01:48:29.539573    control     old_page          0   \n",
       "8   817355  2017-01-04 17:58:08.979471  treatment     new_page          1   \n",
       "9   839785  2017-01-15 18:11:06.610965  treatment     new_page          1   \n",
       "\n",
       "   intercept  ab_page country  CA  UK  US  page_CA  page_UK  page_US  \n",
       "0          1        0      US   0   0   1        0        0        0  \n",
       "1          1        0      US   0   0   1        0        0        0  \n",
       "2          1        1      US   0   0   1        0        0        1  \n",
       "3          1        1      US   0   0   1        0        0        1  \n",
       "4          1        0      US   0   0   1        0        0        0  \n",
       "5          1        0      US   0   0   1        0        0        0  \n",
       "6          1        1      CA   1   0   0        1        0        0  \n",
       "7          1        0      US   0   0   1        0        0        0  \n",
       "8          1        1      UK   0   1   0        0        1        0  \n",
       "9          1        1      CA   1   0   0        1        0        0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate new \"Interaction\" columns.\n",
    "# NOTE: My understanding is that I will need to multiply the 'ab_page' \n",
    "# by each of the 'CA', 'UK', and 'US' columns and then include\n",
    "# two of these three interaction terms in the model set up.\n",
    "df3['page_CA'] = df3['ab_page']*df3['CA']\n",
    "df3['page_UK'] = df3['ab_page']*df3['UK']\n",
    "df3['page_US'] = df3['ab_page']*df3['US']\n",
    "\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290578</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 31 Mar 2021</td> <th>  Pseudo R-squ.:     </th>  <td>3.482e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:16:55</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1920</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -1.9865</td> <td>    0.010</td> <td> -206.344</td> <td> 0.000</td> <td>   -2.005</td> <td>   -1.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0206</td> <td>    0.014</td> <td>   -1.505</td> <td> 0.132</td> <td>   -0.047</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>   -0.0175</td> <td>    0.038</td> <td>   -0.465</td> <td> 0.642</td> <td>   -0.091</td> <td>    0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>   -0.0057</td> <td>    0.019</td> <td>   -0.306</td> <td> 0.760</td> <td>   -0.043</td> <td>    0.031</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>page_CA</th>   <td>   -0.0469</td> <td>    0.054</td> <td>   -0.872</td> <td> 0.383</td> <td>   -0.152</td> <td>    0.059</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>page_UK</th>   <td>    0.0314</td> <td>    0.027</td> <td>    1.181</td> <td> 0.238</td> <td>   -0.021</td> <td>    0.084</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290578\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Wed, 31 Mar 2021   Pseudo R-squ.:               3.482e-05\n",
       "Time:                        13:16:55   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1920\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -1.9865      0.010   -206.344      0.000      -2.005      -1.968\n",
       "ab_page       -0.0206      0.014     -1.505      0.132      -0.047       0.006\n",
       "CA            -0.0175      0.038     -0.465      0.642      -0.091       0.056\n",
       "UK            -0.0057      0.019     -0.306      0.760      -0.043       0.031\n",
       "page_CA       -0.0469      0.054     -0.872      0.383      -0.152       0.059\n",
       "page_UK        0.0314      0.027      1.181      0.238      -0.021       0.084\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up Logistic Regression model with both the respective lower-order and higher-order terms.\n",
    "logit_model = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'CA', 'UK', 'page_CA', 'page_UK']])\n",
    "# REM: \"Baseline\" for 'ab-page' is old_page and for the three countries it is US, as set up here.\n",
    "\n",
    "# Fit and Summarize model.\n",
    "results = logit_model.fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>converted</td>    <th>  No. Observations:  </th>   <td>290584</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>290578</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     5</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 31 Mar 2021</td> <th>  Pseudo R-squ.:     </th>  <td>3.482e-05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:16:56</td>     <th>  Log-Likelihood:    </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>   <td>0.1920</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>   -2.0040</td> <td>    0.036</td> <td>  -55.008</td> <td> 0.000</td> <td>   -2.075</td> <td>   -1.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>   -0.0674</td> <td>    0.052</td> <td>   -1.297</td> <td> 0.195</td> <td>   -0.169</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>    0.0118</td> <td>    0.040</td> <td>    0.296</td> <td> 0.767</td> <td>   -0.066</td> <td>    0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>    0.0175</td> <td>    0.038</td> <td>    0.465</td> <td> 0.642</td> <td>   -0.056</td> <td>    0.091</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>page_UK</th>   <td>    0.0783</td> <td>    0.057</td> <td>    1.378</td> <td> 0.168</td> <td>   -0.033</td> <td>    0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>page_US</th>   <td>    0.0469</td> <td>    0.054</td> <td>    0.872</td> <td> 0.383</td> <td>   -0.059</td> <td>    0.152</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              converted   No. Observations:               290584\n",
       "Model:                          Logit   Df Residuals:                   290578\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Wed, 31 Mar 2021   Pseudo R-squ.:               3.482e-05\n",
       "Time:                        13:16:56   Log-Likelihood:            -1.0639e+05\n",
       "converged:                       True   LL-Null:                   -1.0639e+05\n",
       "Covariance Type:            nonrobust   LLR p-value:                    0.1920\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "intercept     -2.0040      0.036    -55.008      0.000      -2.075      -1.933\n",
       "ab_page       -0.0674      0.052     -1.297      0.195      -0.169       0.034\n",
       "UK             0.0118      0.040      0.296      0.767      -0.066       0.090\n",
       "US             0.0175      0.038      0.465      0.642      -0.056       0.091\n",
       "page_UK        0.0783      0.057      1.378      0.168      -0.033       0.190\n",
       "page_US        0.0469      0.054      0.872      0.383      -0.059       0.152\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up Logistic Regression model with both the respective lower-order and higher-order terms.\n",
    "logit_model = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'UK', 'US', 'page_UK', 'page_US']])\n",
    "# REM: \"Baseline\" for 'ab-page' is old_page and for the three countries it is CA (Canada), as set up here.\n",
    "\n",
    "# Fit and Summarize model.\n",
    "results = logit_model.fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "**If I have correctly set up the preceding \"interaction\" models, none of the p-values associated with the explanatory(x) variable terms showed a significant influence on conversion rate.  In essence, with the interaction terms we are assessing whether \"The way that the 'ab_page' (new_page vs. old_page) is related to the conversion rate is dependent on the 'country' (CA, UK, US) value.\"  Thus, these \"interaction\" models do not appear to argue in favor of a meaningful interaction between the country and the ab_page with respect to conversion rate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Overall Conclusions:\n",
    "1. Potential minor, statistically significant increase in conversion rate in UK compared to CA.\n",
    "2. No other analyses (A/B Test of new page vs. old page or Logistic Regression modeling with explanatory variables of only new page vs. old page or with higher-order interaction terms) indicated any statistically significant difference in conversion rate associated with the landing page, country, or an interaction between these two.\n",
    "3. With respect to practical significance, the tenths of a percent and mixed direction (some positive, but mostly negative) fluctuations in conversion rates associated with the new page argue against implementing the new page.\n",
    "\n",
    "### In summary, neither statistical nor practical considerations favor implementation of the new web page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
